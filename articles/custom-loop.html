<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="luz">
<title>Custom loops with luz • luz</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Custom loops with luz">
<meta property="og:description" content="luz">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">luz</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.1.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Using luz</h6>
    <a class="dropdown-item" href="../articles/get-started.html">Get started</a>
    <a class="dropdown-item" href="../articles/custom-loop.html">Custom loops</a>
    <a class="dropdown-item" href="../articles/accelerator.html">Accelerator API</a>
    <h6 class="dropdown-header" data-toc-skip>Guides</h6>
    <a class="dropdown-item" href="../articles/lr-finder.html">Using the lr_finder</a>
    <a class="dropdown-item" href="../articles/checkpoints.html">Checkpoints models</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/mlverse/luz/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">



<script src="custom-loop_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Custom loops with luz</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/luz/blob/HEAD/vignettes/custom-loop.Rmd" class="external-link"><code>vignettes/custom-loop.Rmd</code></a></small>
      <div class="d-none name"><code>custom-loop.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs" class="external-link">torch</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://mlverse.github.io/luz/" class="external-link">luz</a></span><span class="op">)</span></span></code></pre></div>
<p>Luz is a higher level API for torch that is designed to be highly flexible by providing a layered API that allows it to be useful no matter the level of control your need for your training loop.</p>
<p>In the getting started vignette we have seen the basics of luz and how to quickly modify parts of the training loop using callbacks and custom metrics. In this document we will describe how luz allows the user to get fine-grained control of the training loop.</p>
<p>Apart from the use of callbacks, there are three more ways that you can use luz (depending on how much control you need):</p>
<ul>
<li><p><strong>Multiple optimizers or losses:</strong> You might be optimizing two loss functions each with its own optimizer, but you still don’t want to modify the <code>backward()</code> - <code>zero_grad()</code> and <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code> calls. This is common in models like GANs (Generative Adversarial Networks) when you have competing neural networks trained with different losses and optimizers.</p></li>
<li><p><strong>Fully flexible steps:</strong> You might want to be in control of how to call <code>backward()</code>, <code>zero_grad()</code>and <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code>. You might also want to have more control of gradient computation. For example, you might want to use ‘virtual batch sizes’, where you accumulate the gradients for a few steps before updating the weights.</p></li>
<li><p><strong>Completely flexible loops:</strong> Your training loop can be anything you want but you still want to use luz to handle device placement of the dataloaders, optimizers and models. See <code><a href="../articles/accelerator.html">vignette("accelerator")</a></code>.</p></li>
</ul>
<p>Let’s consider a simplified version of the <code>net</code> that we implemented in the getting started vignette:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">net</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_module.html" class="external-link">nn_module</a></span><span class="op">(</span></span>
<span>  <span class="st">"Net"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">50</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  forward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc1</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc2</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Using the highest level of luz API we would fit it using:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fitted</span> <span class="op">&lt;-</span> <span class="va">net</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/setup.html">setup</a></span><span class="op">(</span></span>
<span>    loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_cross_entropy_loss.html" class="external-link">nn_cross_entropy_loss</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    optimizer <span class="op">=</span> <span class="va">optim_adam</span>,</span>
<span>    metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="va">luz_metric_accuracy</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dl</span>, epochs <span class="op">=</span> <span class="fl">10</span>, valid_data <span class="op">=</span> <span class="va">test_dl</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="multiple-optimizers">Multiple optimizers<a class="anchor" aria-label="anchor" href="#multiple-optimizers"></a>
</h2>
<p>Suppose we want to do an experiment where we train the first fully connected layer using a learning rate of 0.1 and the second one using a learning rate of 0.01. We will minimize the same <code><a href="https://rdrr.io/pkg/torch/man/nn_cross_entropy_loss.html" class="external-link">nn_cross_entropy_loss()</a></code> for both, but for the first layer we want to add L1 regularization on the weights.</p>
<p>In order to use luz for this, we will implement two methods in the <code>net</code> module:</p>
<ul>
<li><p><code>set_optimizers</code>: returns a named list of optimizers depending on the <code>ctx</code>.</p></li>
<li><p><code>loss</code>: computes the loss depending on the selected optimizer.</p></li>
</ul>
<p>Let’s go to the code:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">net</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_module.html" class="external-link">nn_module</a></span><span class="op">(</span></span>
<span>  <span class="st">"Net"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">50</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  forward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc1</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc2</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  set_optimizers <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lr_fc1</span> <span class="op">=</span> <span class="fl">0.1</span>, <span class="va">lr_fc2</span> <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      opt_fc1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/optim_adam.html" class="external-link">optim_adam</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc1</span><span class="op">$</span><span class="va">parameters</span>, lr <span class="op">=</span> <span class="va">lr_fc1</span><span class="op">)</span>,</span>
<span>      opt_fc2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/optim_adam.html" class="external-link">optim_adam</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc2</span><span class="op">$</span><span class="va">parameters</span>, lr <span class="op">=</span> <span class="va">lr_fc2</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  loss <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">input</span>, <span class="va">target</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">pred</span> <span class="op">&lt;-</span> <span class="va">ctx</span><span class="op">$</span><span class="fu">model</span><span class="op">(</span><span class="va">input</span><span class="op">)</span></span>
<span>  </span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">ctx</span><span class="op">$</span><span class="va">opt_name</span> <span class="op">==</span> <span class="st">"opt_fc1"</span><span class="op">)</span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_cross_entropy.html" class="external-link">nnf_cross_entropy</a></span><span class="op">(</span><span class="va">pred</span>, <span class="va">target</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_norm.html" class="external-link">torch_norm</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc1</span><span class="op">$</span><span class="va">weight</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="va">ctx</span><span class="op">$</span><span class="va">opt_name</span> <span class="op">==</span> <span class="st">"opt_fc2"</span><span class="op">)</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_cross_entropy.html" class="external-link">nnf_cross_entropy</a></span><span class="op">(</span><span class="va">pred</span>, <span class="va">target</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Notice that the model optimizers will be initialized according to the <code>set_optimizers()</code> method’s return value (a list). In this case, we are initializing the optimizers using different model parameters and learning rates.</p>
<p>The <code>loss()</code> method is responsible for computing the loss that will then be back-propagated to compute gradients and update the weights. This <code>loss()</code> method can access the <code>ctx</code> object that will contain an <code>opt_name</code> field, describing which optimizer is currently being used. Note that this function will be called once for each optimizer for each training and validation step. See <code><a href="../reference/ctx.html">help("ctx")</a></code> for complete information about the context object.</p>
<p>We can finally <code>setup</code> and <code>fit</code> this module, however we no longer need to specify optimizers and loss functions.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fitted</span> <span class="op">&lt;-</span> <span class="va">net</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="../reference/setup.html">setup</a></span><span class="op">(</span>metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">luz_metric_accuracy</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dl</span>, epochs <span class="op">=</span> <span class="fl">10</span>, valid_data <span class="op">=</span> <span class="va">test_dl</span><span class="op">)</span></span></code></pre></div>
<p>Now let’s re-implement this same model using the slightly more flexible approach of overriding the training and validation step.</p>
</div>
<div class="section level2">
<h2 id="fully-flexible-step">Fully flexible step<a class="anchor" aria-label="anchor" href="#fully-flexible-step"></a>
</h2>
<p>Instead of implementing the <code>loss()</code> method, we can implement the <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code> method. This allows us to flexibly modify what happens when training and validating for each batch in the dataset. You are now responsible for updating the weights by stepping the optimizers and back-propagating the loss.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">net</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_module.html" class="external-link">nn_module</a></span><span class="op">(</span></span>
<span>  <span class="st">"Net"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">50</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">fc1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nn_linear.html" class="external-link">nn_linear</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  forward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc1</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">fc2</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  set_optimizers <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">lr_fc1</span> <span class="op">=</span> <span class="fl">0.1</span>, <span class="va">lr_fc2</span> <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      opt_fc1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/optim_adam.html" class="external-link">optim_adam</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc1</span><span class="op">$</span><span class="va">parameters</span>, lr <span class="op">=</span> <span class="va">lr_fc1</span><span class="op">)</span>,</span>
<span>      opt_fc2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/optim_adam.html" class="external-link">optim_adam</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc2</span><span class="op">$</span><span class="va">parameters</span>, lr <span class="op">=</span> <span class="va">lr_fc2</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  step <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">ctx</span><span class="op">$</span><span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">opt_name</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">ctx</span><span class="op">$</span><span class="va">optimizers</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>      <span class="va">pred</span> <span class="op">&lt;-</span> <span class="va">ctx</span><span class="op">$</span><span class="fu">model</span><span class="op">(</span><span class="va">ctx</span><span class="op">$</span><span class="va">input</span><span class="op">)</span></span>
<span>      <span class="va">opt</span> <span class="op">&lt;-</span> <span class="va">ctx</span><span class="op">$</span><span class="va">optimizers</span><span class="op">[[</span><span class="va">opt_name</span><span class="op">]</span><span class="op">]</span></span>
<span>      <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_cross_entropy.html" class="external-link">nnf_cross_entropy</a></span><span class="op">(</span><span class="va">pred</span>, <span class="va">target</span><span class="op">)</span></span>
<span>      </span>
<span>      <span class="kw">if</span> <span class="op">(</span><span class="va">opt_name</span> <span class="op">==</span> <span class="st">"opt_fc1"</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="co"># we have L1 regularization in layer 1</span></span>
<span>        <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_cross_entropy.html" class="external-link">nnf_cross_entropy</a></span><span class="op">(</span><span class="va">pred</span>, <span class="va">target</span><span class="op">)</span> <span class="op">+</span> </span>
<span>          <span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_norm.html" class="external-link">torch_norm</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">fc1</span><span class="op">$</span><span class="va">weight</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>      <span class="op">}</span></span>
<span>        </span>
<span>      <span class="kw">if</span> <span class="op">(</span><span class="va">ctx</span><span class="op">$</span><span class="va">training</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">opt</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span></span>
<span>        <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span>        <span class="va">opt</span><span class="op">$</span><span class="fu">step</span><span class="op">(</span><span class="op">)</span>  </span>
<span>      <span class="op">}</span></span>
<span>      </span>
<span>      <span class="va">ctx</span><span class="op">$</span><span class="va">loss</span><span class="op">[[</span><span class="va">opt_name</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">loss</span><span class="op">$</span><span class="kw">detach</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The important things to notice here are:</p>
<ul>
<li><p>The <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code> method is used for both training and validation. You need to be careful to only modify the weights when training. Again, you can get complete information regarding the context object using <code><a href="../reference/ctx.html">help("ctx")</a></code>.</p></li>
<li><p><code>ctx$optimizers</code> is a named list holding each optimizer that was created when the <code>set_optimizers()</code> method was called.</p></li>
<li><p>You need to manually track the losses by saving saving them in a named list in <code>ctx$loss</code>. By convention, we use the same name as the optimizer it refers to. It is good practice to <code><a href="https://rdrr.io/r/base/detach.html" class="external-link">detach()</a></code> them before saving to reduce memory usage.</p></li>
<li><p>Callbacks that would be called inside the default <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code> method like <code>on_train_batch_after_pred</code>, <code>on_train_batch_after_loss</code>, etc, won’t be automatically called. You can still cal them manually by adding <code>ctx$call_callbacks("&lt;callback name&gt;")</code> inside your training step. See the code for <code>fit_one_batch()</code> and <code>valid_one_batch</code> to find all the callbacks that won’t be called.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="next-steps">Next steps<a class="anchor" aria-label="anchor" href="#next-steps"></a>
</h2>
<p>In this article you learned how to customize the <code><a href="https://rdrr.io/r/stats/step.html" class="external-link">step()</a></code> of your training loop using luz layered functionality.</p>
<p>Luz also allows more flexible modifications of the training loop described in the Accelerator vignette (<code><a href="../articles/accelerator.html">vignette("accelerator")</a></code>).</p>
<p>You should now be able to follow the examples marked with the ‘intermediate’ and ‘advanced’ category in the <a href="https://mlverse.github.io/luz/articles/examples/index.html" class="external-link">examples gallery</a>.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Daniel Falbel.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
